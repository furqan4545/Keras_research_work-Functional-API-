{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_func_tfdata_api_generatorOnReal_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWCQi9czQcAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBZfoFepQcpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Kp2VEcQxV-",
        "colab_type": "text"
      },
      "source": [
        "# **tf.data API**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yZlik3hQ0wS",
        "colab_type": "text"
      },
      "source": [
        "tf.data() let us handle data of any type\n",
        "\n",
        "we are going to use dataset class for handling all kind of data\n",
        "\n",
        "we can apply all type of preprocessing on dataset object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYRMdiRNQcrc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "194e7611-9831-44ae-858a-be02a70f1eea"
      },
      "source": [
        "# there are different ways of creating dataset. Overhere we are using from_tensor_slices function\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "print(dataset)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (), types: tf.int32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8l1G9E1Qc2a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30adf1cb-af55-4831-e4c1-b72830ce42cb"
      },
      "source": [
        "# there are different ways of creating dataset. Overhere we are using from_tensor_slices function\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices([[1, 2],[3, 4], [5, 6], [7, 8]])\n",
        "print(dataset)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (2,), types: tf.int32>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_jVmjtTQc9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "1d433c1a-e1ff-4903-aa52-08141b0320bf"
      },
      "source": [
        "# we can access each element inside the dataset object. The objet is iterable\n",
        "\n",
        "for elem in dataset:\n",
        "  print(elem)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1 2], shape=(2,), dtype=int32)\n",
            "tf.Tensor([3 4], shape=(2,), dtype=int32)\n",
            "tf.Tensor([5 6], shape=(2,), dtype=int32)\n",
            "tf.Tensor([7 8], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7NBHtLjQdBL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "02b13ef0-2924-43d1-9eb6-6ccd6d58ae66"
      },
      "source": [
        "# we can access each element inside the dataset object. The objet is iterable\n",
        "\n",
        "for elem in dataset:\n",
        "  print(elem.numpy())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2]\n",
            "[3 4]\n",
            "[5 6]\n",
            "[7 8]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exDEZpooQdFj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7863c96-fff6-43e3-859e-094a6287f309"
      },
      "source": [
        "# remember the dataset will always take first dimension as the dataset size. \n",
        "# 2nd argument will be the values or data /samples.\n",
        "# so this dataset will've 128 elements, each of length 5 with random values sampled from uniform distribution.\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.random.uniform([128, 5])) \n",
        "# # 128 rows hongi and 5 cols hongy.\n",
        "print(dataset.element_spec)\n",
        "\n",
        "# the output shows each dataset has length 5 and type float32."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorSpec(shape=(5,), dtype=tf.float32, name=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FO0use2gQdPM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1131d026-45f4-4602-b205-f8b702463904"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform([256, 5], minval = 1, maxval = 10, dtype=tf.int32),\n",
        "     tf.random.normal([256]))\n",
        ")\n",
        "# 256 rows hongi and 5 cols hongy.\n",
        "print(dataset.element_spec)\n",
        "# here we r passing a tuple of tensor to create a dataset\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(5,), dtype=tf.int32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIDxt2qRQdMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ac64ac1b-0855-45af-f38b-741e2980c788"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform([256, 5], minval = 1, maxval = 10, dtype=tf.int32),  # 256 rows and 5 cols\n",
        "     tf.random.normal([256]))    \n",
        ")\n",
        "\n",
        "for elem in dataset.take(2):  # it will take the first 2 elements of the dataset. \n",
        "  print(elem)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 8, 6, 5, 5], dtype=int32)>, <tf.Tensor: shape=(), dtype=float32, numpy=-1.418302>)\n",
            "(<tf.Tensor: shape=(5,), dtype=int32, numpy=array([8, 7, 3, 3, 7], dtype=int32)>, <tf.Tensor: shape=(), dtype=float32, numpy=-0.425806>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ovl4Ni_iQdKh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "2aed10af-173d-48fb-c455-7d67eafb0c8e"
      },
      "source": [
        "tf.random.uniform([256, 5], minval = 1, maxval = 10, dtype=tf.int32)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 5), dtype=int32, numpy=\n",
              "array([[7, 7, 1, 4, 6],\n",
              "       [4, 4, 9, 8, 1],\n",
              "       [7, 6, 4, 1, 1],\n",
              "       ...,\n",
              "       [3, 1, 3, 7, 8],\n",
              "       [8, 9, 4, 5, 6],\n",
              "       [1, 7, 3, 6, 1]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr6Oh1bDmRIB",
        "colab_type": "text"
      },
      "source": [
        "# For e.g.\n",
        "\n",
        "Let's see tf.data on real world dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYb1RGlvQdJQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "50230db0-496b-472e-af0c-e3603cbecad3"
      },
      "source": [
        "# tf.data with cifar10 dataset\n",
        "\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzV6bT4qnTVB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6c76ae82-5da3-493f-86bf-f0c2773848d3"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxuxOCwfQc60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b797730-5737-4623-9302-0586f10e7b6f"
      },
      "source": [
        "# as u can see here from the shape , first dimension is always the length or size of the dataset.\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "print(dataset.element_spec)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(32, 32, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1,), dtype=tf.uint8, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GmNoHFFnxvd",
        "colab_type": "text"
      },
      "source": [
        "we can iterate over this dataset by using for loop to see the data inside"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BxBAdN_Qc5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# we can also pass generator inside ImageDataGenerator.\n",
        "# so lets create generator\n",
        "img_datagen = ImageDataGenerator(width_shift_range=0.2, horizontal_flip = True)\n",
        "\n",
        "# creating dataset object\n",
        "# here we r passing in the generator instead of tensor. we will pass the genrator with .flow callable object. ,flow is used to return generator when called.\n",
        "dataset = tf.data.Dataset.from_generator(img_datagen.flow, args = [X_train, y_train],  # args takes in the input which we usually pass inside the img_datagen generator. we usally pass data and labels.\n",
        "                                         output_types = (tf.float32, tf.int32),   # first type is for inputs and 2nd type is for outputs\n",
        "                                         output_shapes = ([32, 32, 32, 3], [32,1])   # finally here u can see we r giving expected output_shapes for batch of input and output. so qst dim 32 is batch_size \n",
        "                                         )\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5lXV56UQc0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4FMBtexSUft",
        "colab_type": "text"
      },
      "source": [
        "# **Tensorflow Dataset class Coding examples:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVrnVWI_Qcx3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb43n4VVQcwa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d4d19d43-f420-49c3-cda5-04638a0e1a64"
      },
      "source": [
        "# create a ddaaset\n",
        "x = np.zeros((100, 10, 2, 2))\n",
        "\n",
        "# create a dataset from tensor x\n",
        "dataset1 = tf.data.Dataset.from_tensor_slices(x)\n",
        "\n",
        "print(dataset1)\n",
        "print(dataset1.element_spec)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<TensorSliceDataset shapes: (10, 2, 2), types: tf.float64>\n",
            "TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XA1SQ1UMQcuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnUbJbqDQcni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "86e957e1-9baa-4e53-8159-25609dd428a1"
      },
      "source": [
        "x2 = [np.zeros((10,2, 2)), np.zeros((5,2,2))]\n",
        "# this will give error blc the input and output elements are of different shapes.\n",
        "\n",
        "# create a dataset from tensor x\n",
        "dataset2 = tf.data.Dataset.from_tensor_slices(x2)\n",
        "\n",
        "print(dataset2)\n",
        "print(dataset2.element_spec)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-ffa86efed38f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create a dataset from tensor x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \"\"\"\n\u001b[0;32m--> 682\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   2999\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 115\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i))\n\u001b[0m\u001b[1;32m    116\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    336\u001b[0m                                          as_ref=False):\n\u001b[1;32m    337\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m   \"\"\"\n\u001b[1;32m    263\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 264\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Can't convert non-rectangular Python sequence to Tensor."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bG0HmMJuMwA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f059f495-a63b-4146-f0e6-65f1dbde63d7"
      },
      "source": [
        "x2 = np.zeros((10,2, 2)) \n",
        "x2.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6qdaiW7uM5P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d935e296-37a4-4f76-e622-457aa4f5160f"
      },
      "source": [
        "x2 = [np.zeros((10,2, 2)), np.zeros((5,2,2))]\n",
        "x2[0].shape  # ak hi list me 2 array store hain."
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehBN9zUsuM2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now it will work blc now the shape is ok\n",
        "x2 = [np.zeros((10, 1)), np.zeros((10, 1)), np.zeros((10, 1))]"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyfcMd5-uM0a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "531da811-9dc5-4de7-c45c-3479a0d9be11"
      },
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(x2)\n",
        "print(dataset2.element_spec)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorSpec(shape=(10, 1), dtype=tf.float64, name=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIJ-t0OmuMzL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmnrsmrF5XUp",
        "colab_type": "text"
      },
      "source": [
        "## **Combining two datasets into larger dataset, done with the help of Zipping**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbaLBx4vuQWQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50928081-1fc0-486e-f61f-45fd7363da99"
      },
      "source": [
        "# Combine 2 datasets into larger one\n",
        "# Note : The two datasets that we are zipping needs to be in the same shape. \n",
        "\n",
        "dataset_zipped = tf.data.Dataset.zip((dataset1, dataset2)) # we will pass two datasets that we want to combine as tuple.\n",
        "\n",
        "print(dataset_zipped.element_spec)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(10, 2, 2), dtype=tf.float64, name=None), TensorSpec(shape=(10, 1), dtype=tf.float64, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqli1XxAuQZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a funct to find the num of batches in a dataset zipped\n",
        "\n",
        "def get_batches(dataset):\n",
        "  iter_dataset = iter(dataset)\n",
        "  i = 0\n",
        "  try:\n",
        "    while next(iter_dataset):\n",
        "      i = i + 1\n",
        "\n",
        "  except:\n",
        "    return i\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbFc-Gr5uQS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "afaa9566-7137-448b-e7cb-d17057d485df"
      },
      "source": [
        "# find the no of batches in the zipped dataset\n",
        "\n",
        "get_batches(dataset_zipped)\n",
        "# 3 is the no of batches of smaller dataset in the zipped_dataset i.e. dataset2\n",
        "# notice if u do zip_dataset with no of different batches then the dataset with larget no of batches will be trimmed in order to accomodate smaller dataset.  "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YCvDhmUuMuF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5addcecc-d0e5-4990-a3b5-1f2d7a176eb4"
      },
      "source": [
        "# creating dataset from real data.\n",
        "\n",
        "# load the mnist dataset\n",
        "\n",
        "(train_features, train_labels), (test_features, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(type(train_features), type(train_labels))\n",
        "print(train_features.shape, train_labels.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "(60000, 28, 28) (60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pYYwuP9uMry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataset from the mnist data\n",
        "\n",
        "mnist_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4tzIe_1uMos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c18384e7-5de1-464e-c8f3-05b975085580"
      },
      "source": [
        "# Inspect the Dataset opbject\n",
        "\n",
        "print(mnist_dataset.element_spec)\n",
        "# resultant dataset is tuple. first element is the element of training features where the second element is the element of training label. "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(28, 28), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.uint8, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gubiGPyX9dJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8fd48c03-9c11-4369-ab46-668bf8fc4bbe"
      },
      "source": [
        "# Inspect the lenght of an element using the take method\n",
        "\n",
        "element = next(iter(mnist_dataset.take(1)))\n",
        "print(len(element))\n",
        "# blc we have 2 datasets.\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viTBlAxz9dTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "25c93f69-5671-40a0-b883-a31d7d68713b"
      },
      "source": [
        "# converting the shapes of our data\n",
        "print(element[0].shape)\n",
        "print(element[1].shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(28, 28)\n",
            "()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ImfEfR19dWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os3llYjv-z6u",
        "colab_type": "text"
      },
      "source": [
        "# ***Create a dataset from the text data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjHn__719dRg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "8ff1cf37-3537-41a2-a733-23d8884fa532"
      },
      "source": [
        "# print the list of text files.\n",
        "\n",
        "text_files = sorted([f.path for f in os.scandir('/content/drive/My Drive/datasets/shakespeare')])\n",
        "\n",
        "text_files"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/datasets/shakespeare/tempest.1.1.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.1.2.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.2.1.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.2.2.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.3.1.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.3.2.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.3.3.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.4.1.txt',\n",
              " '/content/drive/My Drive/datasets/shakespeare/tempest.5.1.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO6liQyS9dPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsWgnuqz9dNS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "dff81695-a75c-43fa-d3cc-b41be241f2c0"
      },
      "source": [
        "# load the first file using python and print the first 5 lines. \n",
        "\n",
        "with open(text_files[0], 'r') as fil:\n",
        "  contents = (fil.readline() for i in range(5))\n",
        "  for line in contents:\n",
        "    print(line)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SCENE I. On a ship at sea: a tempestuous noise\n",
            "\n",
            "of thunder and lightning heard.\n",
            "\n",
            "Enter a Master and a Boatswain\n",
            "\n",
            "\n",
            "\n",
            "Master\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYxkMS9H9dL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the lines from the files into a dataset using TextLineDataset\n",
        "\n",
        "shakespare_dataset = tf.data.TextLineDataset(text_files)\n",
        "# you can pass here one file or multiple files. It's upto u. "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ID6nZcFq9dHM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3b4dc63a-f79b-4d76-de97-50fdd9a85309"
      },
      "source": [
        "# Use the take method to get and print the first 5 lines of the dataset\n",
        "\n",
        "first_5_lines_dataset = iter(shakespare_dataset.take(5)) # take 5 elements in the dataset from the first file. take() is used to iterate first element in the dataset\n",
        "lines = [line for line in first_5_lines_dataset]\n",
        "for line in lines:\n",
        "    print(line)\n",
        "\n",
        "# here we r loading the first 5 lines of the first file. does that mean we r not loading rest of the files.. we can check that \n",
        "# in the below code."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'SCENE I. On a ship at sea: a tempestuous noise', shape=(), dtype=string)\n",
            "tf.Tensor(b'of thunder and lightning heard.', shape=(), dtype=string)\n",
            "tf.Tensor(b'Enter a Master and a Boatswain', shape=(), dtype=string)\n",
            "tf.Tensor(b'', shape=(), dtype=string)\n",
            "tf.Tensor(b'Master', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UFcGtI99dE6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b557c4e-9e30-42fe-ffc6-17b4bf8c6b07"
      },
      "source": [
        "# compute the number of lines in the first file\n",
        "\n",
        "lines = []\n",
        "with open(text_files[0], 'r') as fil:\n",
        "  line = fil.readline()\n",
        "  while line:\n",
        "    # print(line)\n",
        "    lines.append(line)\n",
        "    line = fil.readline() # this mean we will read new line everytime in order to avoid repeatation.\n",
        "print(len(lines))\n",
        "\n",
        "# just blc our files are loaded sequentially so therefore we wont be able to see rest of the files data in atleast 121 iteration/elements. blc in the starting\n",
        "# 121 elements we got data of file1. after that data of file2 starts. \n",
        "# So it's like all the files data is merged together in one list.  "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dKeblbUQclH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f5e5fe8a-f658-4b89-fd17-f1745c426ad9"
      },
      "source": [
        "# now we will iterate over the entire dataset that we created. i.e. all the files\n",
        "# Compute the number of lines in the shakespeare dataset we created\n",
        "\n",
        "shakespeare_dataset_iterator = iter(shakespare_dataset)  # here we r not using take() so our interator will iterate over the all files. \n",
        "lines = [line for line in shakespeare_dataset_iterator]\n",
        "print(len(lines))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrJSa0i1B9xR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8fe08496-b880-4968-ee85-e8b33cd56601"
      },
      "source": [
        "files = next(iter(shakespare_dataset))\n",
        "print(files)\n",
        "\n",
        "allfile = []\n",
        "a = iter(shakespare_dataset)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'SCENE I. On a ship at sea: a tempestuous noise', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=string, numpy=b'SCENE I. On a ship at sea: a tempestuous noise'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdLMZRLcD8dA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c410649-c93f-4f7f-f128-21454a66236f"
      },
      "source": [
        "ex_line = next(a)   # next evertime prints a new line until last line is reached\n",
        "print(ex_line.numpy())"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Good, speak to the mariners: fall to't, yarely,\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrKzKFTgA24R",
        "colab_type": "text"
      },
      "source": [
        "#### Interleave lines from the text data files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukfmUlQkAkwY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "11ff7111-ecfe-4580-8260-9aabb5c12f31"
      },
      "source": [
        "# how can we overcome the problem of merging the files data. i.e. we want to do work on every file data seperately, for taht we use interleave function.\n",
        "\n",
        "# Create a dataset of the text file strings\n",
        "\n",
        "text_files_dataset = tf.data.Dataset.from_tensor_slices(text_files)\n",
        "files = [file for file in text_files_dataset]\n",
        "for file in files:\n",
        "    print(file)\n",
        "\n",
        "# these r the list of files we have in our text_file array."
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.1.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.1.2.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.2.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.2.2.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.3.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.3.2.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.3.3.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.4.1.txt', shape=(), dtype=string)\n",
            "tf.Tensor(b'/content/drive/My Drive/datasets/shakespeare/tempest.5.1.txt', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkujNujDAku7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a4029ef-3914-4c85-a22a-edb89b9c808f"
      },
      "source": [
        "# we can now use the interleave function to create a new dataset from the dataset of the text file so that interleave function takes mapping function which has to \n",
        "# convert element into a dataset in this case text line  dataset. \n",
        "# cycle_length is the length of datasets which we want to interleave. \n",
        "# Interleave the lines from the text files\n",
        "\n",
        "interleaved_shakespare_dataset = text_files_dataset.interleave(tf.data.TextLineDataset, cycle_length=9)\n",
        "print(interleaved_shakespare_dataset)\n"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<InterleaveDataset shapes: (), types: tf.string>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCL8HYqSAktq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "955a4e48-fcb7-41bb-91fe-624d2683eab4"
      },
      "source": [
        "# Print the first 10 elements of the interleaved dataset\n",
        "\n",
        "lines = [line for line in iter(interleaved_shakespare_dataset.take(10))]\n",
        "for line in lines:\n",
        "    print(line)\n",
        "\n",
        "# these are 10 lines from 9 different files. i.e. first 9 lines are the first line from every single file. and 10th line is the 2nd line of the first file,\n",
        "# simlarly if we would have printed more lines then it would have printed 2nd line of the other files too.  "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'SCENE I. On a ship at sea: a tempestuous noise', shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE II. The island. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE I. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE II. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE I. Before PROSPERO'S Cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE II. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b'SCENE III. Another part of the island.', shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE I. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b\"SCENE I. Before PROSPERO'S cell.\", shape=(), dtype=string)\n",
            "tf.Tensor(b'of thunder and lightning heard.', shape=(), dtype=string)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaE-EZBeAkq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbzKc9-xAkoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}